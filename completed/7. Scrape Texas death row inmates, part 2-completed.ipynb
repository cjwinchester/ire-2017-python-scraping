{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape Texas death row inmates, part 2\n",
    "\n",
    "The table we scraped in the last notebook _probably_ could have been imported directly into Excel without too much trouble. But what if you also wanted to append a few columns of information from each inmate's detail page, as well?\n",
    "\n",
    "In this section, we're going to supplement the scraper we just wrote with a _function_ that extracts data from inmates' detail pages. We're also going to use Python's built-in `time.sleep` function to pause for a few seconds between each row to give the government's servers a break.\n",
    "\n",
    "First, let's import the libraries we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's write a function\n",
    "\n",
    "We need a function that will take a URL of a detail page and do these things:\n",
    "\n",
    "- Open the detail page URL using `requests`\n",
    "- Parse the contents using `BeautifulSoup`\n",
    "- Isolate the bits of information we're interested in: height, weight, eye color, hair color, native county, native state, link to mugshot\n",
    "- Return those bits of information to the script that called the function -- let's use a dictionary\n",
    "\n",
    "We shall call our function `inmateDetails()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inmateDetails(url):\n",
    "\n",
    "    # create a dictionary with some default values\n",
    "    out_dict = {\n",
    "        'Height': None,\n",
    "        'Weight': None,\n",
    "        'Eye Color': None,\n",
    "        'Hair Color': None,\n",
    "        'Native County': None,\n",
    "        'Native State': None,\n",
    "        'mug': None\n",
    "    }\n",
    "    \n",
    "    if not url.endswith('.html'):\n",
    "        return out_dict\n",
    "    \n",
    "    # get the page\n",
    "    r = requests.get(url)\n",
    "    \n",
    "    # soup the HTML\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # find the table of info\n",
    "    table = soup.find('table', {'class': 'tabledata_deathrow_table'})\n",
    "    \n",
    "    # target the mugshot, if it exists\n",
    "    mug = table.find('img', {'class': 'photo_border_black_right'})\n",
    "    \n",
    "    # if there is a mug, grab the src and add it to the dict\n",
    "    if mug:\n",
    "        out_dict['mug'] = 'http://www.tdcj.state.tx.us/death_row/dr_info/' + mug['src']\n",
    "\n",
    "    # get a list of the \"label\" cells\n",
    "    # on some pages, they're identified by the class 'tabledata_bold_align_right_deathrow'\n",
    "    # on others, they're identified by the class 'tabledata_bold_align_right_unit'\n",
    "    # so we need to see which one we're working with\n",
    "    label_cells = table.find_all('td', {'class': 'tabledata_bold_align_right_deathrow'})\n",
    "    \n",
    "    if not label_cells:\n",
    "        label_cells = table.find_all('td', {'class': 'tabledata_bold_align_right_unit'})        \n",
    "    \n",
    "    # a list of the things we're interested in -- should match exactly the text of the cells\n",
    "    attr_list = ['Height', 'Weight', 'Eye Color', 'Hair Color', 'Native County', 'Native State']\n",
    "\n",
    "    # loop over the list of label cells\n",
    "    for cell in label_cells:\n",
    "        \n",
    "        # check to see if the cell text is in our list of attributes\n",
    "        if cell.text.strip() in attr_list:\n",
    "            \n",
    "            # if so, find the value -- go up to the tr and search for the other td --\n",
    "            # and add that attribute to our dictionary\n",
    "            out_dict[cell.text.strip()] = cell.parent.find('td', {'class': 'tabledata_align_left_deathrow'}).text.strip()\n",
    "\n",
    "    # return the dictionary to the script\n",
    "    return(out_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "OK, now we have our function. Let's drop it in the scraper we wrote for the last session.\n",
    "\n",
    "First, let's get back to the part where where we have the rows of the table stored as a variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://www.tdcj.state.tx.us/death_row/dr_offenders_on_dr.html'\n",
    "\n",
    "r = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "dr_table = soup.find('table', {'class': 'os'})\n",
    "\n",
    "dr_rows = dr_table.find_all('tr')[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to loop over the rows of the table again as we write to a file -- let's call it 'tx-death-row-with-details.csv' -- but this time, we're _also_ going to call the function we just wrote, `inmateDetails`, to grab some details from the detail page.\n",
    "\n",
    "The details will be returned as a dictionary, and we'll add these values to the list that we write out to file instead of just dropping in the link to the detail page.\n",
    "\n",
    "_Furthermore_, because we're adding an HTTP request to every loop iteration, we're going to use `time.sleep` to pause for a few seconds at the end of each loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999604\n",
      "999603\n",
      "999602\n",
      "999601\n",
      "999600\n",
      "999599\n",
      "999598\n",
      "999597\n",
      "999596\n",
      "999595\n",
      "999594\n",
      "999593\n",
      "999592\n",
      "999591\n",
      "999590\n",
      "999589\n",
      "999588\n",
      "999587\n",
      "999586\n",
      "999585\n",
      "999584\n",
      "999582\n",
      "999581\n",
      "999580\n",
      "999579\n",
      "999578\n",
      "999577\n",
      "999576\n",
      "999575\n",
      "999573\n",
      "999572\n",
      "999571\n",
      "999570\n",
      "999569\n",
      "999568\n",
      "999567\n",
      "999566\n",
      "999565\n",
      "999564\n",
      "999563\n",
      "999562\n",
      "999561\n",
      "999560\n",
      "999559\n",
      "999558\n",
      "999557\n",
      "999556\n",
      "999554\n",
      "999553\n",
      "999551\n",
      "999550\n",
      "999549\n",
      "999548\n",
      "999547\n",
      "999545\n",
      "999544\n",
      "999543\n",
      "999542\n",
      "999541\n",
      "999538\n",
      "999537\n",
      "999536\n",
      "999535\n",
      "999534\n",
      "999531\n",
      "999529\n",
      "999527\n",
      "999526\n",
      "999524\n",
      "999523\n",
      "999522\n",
      "999520\n",
      "999519\n",
      "999518\n",
      "999517\n",
      "999516\n",
      "999515\n",
      "999514\n",
      "999513\n",
      "999512\n",
      "999509\n",
      "999508\n",
      "999507\n",
      "999506\n",
      "999505\n",
      "999502\n",
      "999501\n",
      "999498\n",
      "999497\n",
      "999495\n",
      "999494\n",
      "999493\n",
      "999492\n",
      "999490\n",
      "999489\n",
      "999488\n",
      "999484\n",
      "999482\n",
      "999480\n",
      "999477\n",
      "999476\n",
      "999473\n",
      "999472\n",
      "999469\n",
      "999465\n",
      "999464\n",
      "999462\n",
      "999461\n",
      "999460\n",
      "999459\n",
      "999458\n",
      "999455\n",
      "999453\n",
      "999450\n",
      "999447\n",
      "999446\n",
      "999443\n",
      "999442\n",
      "999441\n",
      "999436\n",
      "999433\n",
      "999423\n",
      "999420\n",
      "999416\n",
      "999412\n",
      "999411\n",
      "999410\n",
      "999406\n",
      "999402\n",
      "999399\n",
      "999398\n",
      "999396\n",
      "999393\n",
      "999392\n",
      "999391\n",
      "999390\n",
      "999388\n",
      "999386\n",
      "999383\n",
      "999381\n",
      "999379\n",
      "999376\n",
      "999373\n",
      "999371\n",
      "999369\n",
      "999366\n",
      "999361\n",
      "999354\n",
      "999351\n",
      "999333\n",
      "999332\n",
      "999331\n",
      "999330\n",
      "999328\n",
      "999325\n",
      "999319\n",
      "999315\n",
      "999308\n",
      "999306\n",
      "999305\n",
      "999300\n",
      "999299\n",
      "999295\n",
      "999291\n",
      "999290\n",
      "999283\n",
      "999282\n",
      "999279\n",
      "999275\n",
      "999271\n",
      "999267\n",
      "999260\n",
      "999259\n",
      "999258\n",
      "999256\n",
      "999248\n",
      "999247\n",
      "999240\n",
      "999235\n",
      "999228\n",
      "999226\n",
      "999220\n",
      "999208\n",
      "999207\n",
      "999205\n",
      "999204\n",
      "999203\n",
      "999202\n",
      "999192\n",
      "999189\n",
      "999181\n",
      "999174\n",
      "999164\n",
      "999144\n",
      "999143\n",
      "999137\n",
      "999110\n",
      "999109\n",
      "999108\n",
      "999105\n",
      "999102\n",
      "999075\n",
      "999062\n",
      "999051\n",
      "999049\n",
      "999046\n",
      "999044\n",
      "999040\n",
      "999036\n",
      "999032\n",
      "999020\n",
      "999019\n",
      "999006\n",
      "999000\n",
      "993\n",
      "992\n",
      "976\n",
      "956\n",
      "928\n",
      "925\n",
      "910\n",
      "909\n",
      "873\n",
      "869\n",
      "868\n",
      "866\n",
      "856\n",
      "778\n",
      "768\n",
      "736\n",
      "663\n",
      "650\n",
      "636\n",
      "609\n",
      "577\n",
      "541\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "with open('tx-death-row-with-details.csv', 'w') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    headers = ['id', 'last', 'first', 'dob', 'sex', 'race',\n",
    "               'admission_date', 'county', 'offense_date',\n",
    "               'height', 'weight', 'eye_color', 'hair_color',\n",
    "               'native_county', 'native_state']\n",
    "    \n",
    "    writer.writerow(headers)\n",
    "    \n",
    "    for row in dr_rows:\n",
    "        cols = row.find_all('td')\n",
    "\n",
    "        id_number = cols[0].text\n",
    "        \n",
    "        print(id_number)\n",
    "        last_name = cols[2].text\n",
    "        first_name = cols[3].text\n",
    "        dob = cols[4].text\n",
    "        sex = cols[5].text\n",
    "        race = cols[6].text\n",
    "        date_received = cols[7].text\n",
    "        county = cols[8].text\n",
    "        date_offense = cols[9].text\n",
    "\n",
    "        detail_link = 'http://www.tdcj.state.tx.us/death_row/' + cols[1].a['href']\n",
    "        details = inmateDetails(detail_link)\n",
    "        \n",
    "        height = details['Height']\n",
    "        weight = details['Weight']\n",
    "        eye_color = details['Eye Color']\n",
    "        hair_color = details['Hair Color']\n",
    "        native_county = details['Native County']\n",
    "        native_state = details['Native State']\n",
    "        \n",
    "        writer.writerow([id_number, detail_link, last_name, first_name, dob, sex,\n",
    "                         race, date_received, county, date_offense, height, weight,\n",
    "                         eye_color, hair_color, native_county, native_state])\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
